\chapter{Теоретические основы анализа текстовых данных}

В данной главе описаны методы и концепции, применяющиеся в
задачах анализа текста, так как они используются в подходе
к решению поставленной задачи (см. главу~\ref{ch:problem_solving}).

В разделе~\ref{sec:term_document_matrix} приводится описание
концепции <<матрица “термин-документ”>>.

В разделе~\ref{sec:latent_semantic_analysis} описан подход,
именуемый <<латентным семантическим анализом>>.

В разделе~\ref{sec:latent_semantic_analysis} описан подход,
который называется <<векторное представление слов>>.

\section{Матрица <<термин-документ>>}
\label{sec:term_document_matrix}

Матрица <<термин-документ>> является одной из ключевых концепцией,
использующихся в дисциплине текстового информационного 
поиска~\cite{manning2008introduction}. 

\textit{Термином} называется некоторая атомарная лингвистическая
единица в языке. Обычно терминами являются слова, поэтому иногда
говорят \textit{слова}, подразумевая под этим понятием термины.

\textit{Документом} называется некоторая конечная последовательность
терминов. Документами в зависимости от области применения данной концепции
могут являться книги, статьи или веб-страницы.

\textit{Коллекцией} называется некоторое конечное множество документов.

Пусть имеется коллекция документов:
\[
    \mathcal{D} = \{\mathcal{D}_1, \mathcal{D}_2,...,\mathcal{D}_n\}.
\]
Введём множество всех терминов, присутствующих в коллекции (\textit{словарь}):
\[
    T = \{t \colon t \in \bigcup_{j=1}^{n} \mathcal{D}_j\} = \{t_i\}_{i=1}^{m}.
\]
Подход заключается в том, чтобы описать данные матрицей 
$D \in \mathbb{R}^{m \times n}$, каждый элемент $d_{ij}$ которой будет
означать <<степень принадлежности>> термина $i$ документу $j$. То есть,
чем больше значение элемента $d_{ij}$, тем лучше термин $i$ <<описывает>>
документ $j$. Таким образом, столбцы матрицы $D$ являются векторным
представлением документов.

Для задач, связанных с анализом текста, используется принцип
<<bag of words>>~\cite{manning2008introduction2}. Данный принцип
заключается в том, что порядок следования терминов в документе
не имеет никакого значения. Для описания формул, по которым вычисляются 
элементы матрицы <<термин-документ>> на основе принципа <<bag of words>>,
введём вспомогательные величины:
\[
    \mathrm{tf}_{ij} = \sum_{t \in \mathcal{D}_j} [t_i = t],
\]
\[
    \mathrm{df}_{i} = \sum_{j=1}^{n} [t_i \in \mathcal{D}_j],
\]
\[
    \mathrm{gf}_{i} = \sum_{j=1}^{n} \mathrm{tf}_{ij},
\]
где выражение $[\cdot]$ равно единице, если внутри находится истинный
предикат, иначе выражение равно нулю. Таким образом $\mathrm{tf}_{ij}$~---
число встреч термина $i$ в документе $j$, $\mathrm{df}_{i}$~---
число документов в которых встречается термин $i$, а $\mathrm{gf}_{i}$~---
число встреч термина $i$ во всей коллекции.

Обычно для вычисления элементов матрицы <<термин-документ>> используется
\textit{формула TF-IDF}, которая имеет следующий вид:
\begin{equation}\label{eq:tf_idf}
    d_{ij} = \mathrm{tf}_{ij} \cdot \log{\frac{n}{\mathrm{df}_{i}}}.
\end{equation}
Формулу TF-IDF естественным образом можно обобщить:
\begin{equation}\label{eq:general_tfidf}
d_{ij} = \begin{cases}
    0,& \mathrm{tf}_{ij} = 0,\\
    l(\mathrm{tf}_{ij}) \cdot g(\frac{n}{\mathrm{df}_{i}}),& \mathrm{tf}_{ij} \ne 0,
         \end{cases}
\end{equation}
где $f$ и $g$~--- произвольные неотрицательные неубывающие функции определённые
на промежутке $\left[1, +\infty \right]$. В
таблице~\ref{tab:f_g_examples} приведены примеры функций $f$ и $g$,
которые могут быть использованы.

\begin{table}[!h]
    \caption{Примеры функций $f$ и $g$ в обобщённой формуле TF-IDF}\label{tab:f_g_examples}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}\hline
    \boldmath$f(x)$ & $1$ & $\log{x}$ & $\log{x}$ & $\log{x}$ & $\sqrt{x}$ & $\sqrt{x}$ & $\sqrt{x}$ & $x$ & $x$ & $x$ \\\hline
    \boldmath$g(x)$ & $1$ & $1$ & $\log{x}$ & $\sqrt{x}$ & $1$ & $\log{x}$ & $\sqrt{x}$ & $1$ & $\log{x}$ & $\sqrt{x}$ \\\hline
\end{tabular}
\end{table}

Ещё одним распространённым способом вычисления коэффициентов
матрицы <<термин-документ>> является формула \textit{log-entropy}.
Она имеет следующий вид:
\begin{equation}\label{eq:log_entropy}
    d_{ij} = \log{(\mathrm{tf}_{ij} + 1)} \cdot
    (1 + \sum_{j=1}^{n} \frac{p_{ij} \cdot \log{p_{ij}}}{\log{n}}),\quad
    p_{ij} = \frac{\mathrm{tf}_{ij}}{\mathrm{gf}_i}.
\end{equation}

Существуют также и другие известные способы вычисления элементов
матрицы <<термин-документ>>, но так как в рамках настоящего исследования
они не использовались, их рассмотрение будет опущено.

\section{Латентный семантический анализ}
\label{sec:latent_semantic_analysis}
\textit{Латентный семантический анализ}~--- это некоторый подход,
часто применяющийся в задачах анализа текста. Он основан на
предположении о том, что термины и документы могут быть отнесены
к некоторым тематикам, поэтому данный подход также называют
\textit{тематическим моделированием}.

Опишем задачу, которую решает латентный семантический анализ,
более формально. Пусть $D \in \mathbb{R}^{m \times n}$~--- матрица 
<<термин-документ>>, вычисленная каким-либо образом. Требуется
выполнить следующее разложение данной матрицы:
\[
    D = U \cdot V^T,\quad U \in \mathbb{R}^{m \times k},\quad V \in \mathbb{R}^{n \times k},
\]
где $U$~--- матрица <<термин-тема>>, $V$~--- матрица <<документ-тема>>,
а $k$~--- число тем. Строка матрицы $U$ под номером $i$ 
характеризует <<степень принадлежности>> термина $i$ 
каждой из тем. Строка матрицы $V$ под номером $j$ обозначает
<<степень принадлежности>> документа $j$ каждой из тем.
Можно сказать, что латентный семантический анализ решает
две задачи нечёткой кластеризации (fuzzy clustering)~--- для
терминов и для документов.

В виду того, что число терминов, как правило,
оказывается очень большим, латентный семантический анализ
применяется для более компактного представления 
документов в векторном пространстве. Кроме того, данный подход
применим для категоризации документов.

Алгоритмы, решающие задачу латентного семантического анализа,
можно разделить на две группы: вероятностные и остальные.

Алгоритмы, относящиеся к первой группе, реализуются на
основе вероятностной модели порождения данных, которая
формально записывается следующим образом:
\[
    p(d, w) = \sum_{t \in T} p(t)p(w|t)p(d|t),
\]
где $T$~--- множество тем, $p(d, w)$~--- вероятность возникновения
термина $w$ в документе $d$, $p(t)$~--- вероятность выбрать тему $t$,
$p(w|t)$~--- вероятность выбрать термин $w$ из темы $t$, а
$p(d|t)$~--- вероятность выбрать документ $d$, при условии, что
выбрана тема $t$.

В алгоритмах, основанных на вероятностной модели порождения
данных, элементами матрицы являются вероятности.

Одним из вероятностных алгоритмов латентного семантического анализа
является \textit{probabalistic latent semantic analysis 
(PLSA)}~\cite{chemudugunta2007modeling}. Другим, наиболее
популярным вероятностным алгоритмом является \textit{latent
Dirichlet allocation (LDA)}~\cite{blei2003latent}.

Из алгоритмов латентного семантического анализа, не относящихся
к вероятностным, следует упомянуть 
\textit{latent semantic indexing (LSI)}~\cite{deerwester1990indexing}.
Данный алгоритм реализован на основе сингулярного разложения,
поэтому он позволяет снижать размерность сильно разреженных
матриц (какими часто являются матрицы <<термин-документ>>) с
минимальным потерями информации. Достоинством данного алгоритма
является тот факт, что он не завязан на предположении о
существовании латентных тем. Главный же его недостаток~--- 
полученные темы являются неинтерпритируемыми для человека.
Вероятностные модели лишены этого недостатка.

\section{Векторное представление слов}
\label{sec:word_embedding}
\textit{Векторное представление слов} (word embedding)~---
техника, позволяющая преобразовать множество терминов в
численные векторы.

Пусть $\mathcal{D}$~--- коллекция документов, а
$T$~--- словарь терминов. Тогда алгоритм $\mathcal{A}$,
реализующий word embedding, можно рассматривать следующим образом:
\[
    \mathcal{A}(\mathcal{D}, k) \colon T \mapsto \mathbb{R}^k.
\]
Иными словами, техника word embedding позволяет сопоставить
каждому термину вектор заданной размерности $k$, основываясь
на коллекции документов.

Техника word embedding предоставляет возможность производить
операции сравнения над терминами, для чего обычно используется
косинусная мера схожести или евклидова метрика. Это позволяет
находить близкие по семантике термины или разбить множество
терминов на кластеры.

Рассмотрим наиболее популярный алгоритм, который реализует
технику векторного представления слов,~--- 
\textit{Word2Vec}~\cite{goldberg2014word2vec}. Данный
алгоритм не придерживается предположения <<bag of words>>.
Вектора терминов, которые являются выходными данными алгоритма,
описывают каждый термин через его лингвистический \textit{контекст}.
Контекстом конкретного термина в документе, называется $n$ терминов,
расположенных до данного термина, и $n$ терминов, следующих за ним.
На ряду с размерностью векторного пространства $k$, значение $n$
также является параметром этого алгоритма.

\chapterconclusion

В настоящей главе были рассмотрены некоторые методы и концепции,
которые используются в задачах, связанных с анализом текстовой
информации. Описанные методы и концепции будут использоваться
при дальнейшем повествовании.
