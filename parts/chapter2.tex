\chapter{Теоретические основы анализа текстовых данных}

В данной главе описаны методы и концепции, применяющиеся в
задачах анализа текста, так как они используются в подходе
к решению поставленной задачи (см. главу~\ref{ch:problem_solving}).

В разделе~\ref{sec:term_document_matrix} приводится описание
концепции <<матрица “термин-документ”>>.

В разделе~\ref{sec:latent_semantic_analysis} описан подход,
именуемый <<латентным семантическим анализом>>.

В разделе~\ref{sec:latent_semantic_analysis} описан подход,
который называется <<векторное представление слов>>.

\section{Матрица <<термин-документ>>}
\label{sec:term_document_matrix}

Матрица <<термин-документ>> является одной из ключевых концепцией,
использующихся в дисциплине текстового информационного 
поиска~\cite{manning2008introduction}. 

\textit{Термином} называется некоторая атомарная лингвистическая
единица в языке. Обычно терминами являются слова, поэтому иногда
говорят \textit{слова}, подразумевая под этим понятием термины.

\textit{Документом} называется некоторая конечная последовательность
терминов. Документами в зависимости от области применения данной концепции
могут являться книги, статьи или веб-страницы.

\textit{Коллекцией} называется некоторое конечное множество документов.

Пусть имеется коллекция документов:
\[
    \mathcal{D} = \{\mathcal{D}_1, \mathcal{D}_2,...,\mathcal{D}_n\}.
\]
Введём множество всех терминов, присутствующих в коллекции:
\[
    T = \{t \colon t \in \bigcup_{j=1}^{n} \mathcal{D}_j\} = \{t_i\}_{i=1}^{m}.
\]
Подход заключается в том, чтобы описать данные матрицей 
$D \in \mathbb{R}^{m \times n}$, каждый элемент $d_{ij}$ которой будет
означать <<степень принадлежности>> термина $i$ документу $j$. То есть,
чем больше значение элемента $d_{ij}$, тем больше термин $i$ <<описывает>>
документ $j$. Таким образом, столбцы матрицы $D$ являются векторным
представлением документов.

Элементы данной матрицы могут вычисляться различными способами. Для описания
формул, по которым вычисляются эти элементы, введём вспомогательные величины:
\[
    \mathrm{tf}_{ij} = \sum_{t \in \mathcal{D}_j} [t_i = t],
\]
\[
    \mathrm{df}_{i} = \sum_{j=1}^{n} [t_i \in \mathcal{D}_j],
\]
\[
    \mathrm{gf}_{i} = \sum_{j=1}^{n} \mathrm{tf}_{ij},
\]
где выражение $[\cdot]$ равно единице, если внутри находится истинный
предикат, иначе выражение равно нулю. Таким образом $\mathrm{tf}_{ij}$~---
число встреч термина $i$ в документе $j$, $\mathrm{df}_{i}$~---
число документов в которых встречается термин $i$, а $\mathrm{gf}_{i}$~---
число втреч термина $i$ во всей коллекции.

Обычно для вычисления элементов матрицы <<термин-документ>> используется
\textit{формула TF-IDF}, которая имеет следующий вид:
\begin{equation}\label{eq:tf_idf}
    d_{ij} = \mathrm{tf}_{ij} \cdot \log{\frac{n}{\mathrm{df}_{i}}}.
\end{equation}
Формулу TF-IDF естественным образом можно обобщить:
\begin{equation}\label{eq:general_tfidf}
d_{ij} = \begin{cases}
    0,& \mathrm{tf}_{ij} = 0,\\
    l(\mathrm{tf}_{ij}) \cdot g(\frac{n}{\mathrm{df}_{i}}),& \mathrm{tf}_{ij} \ne 0,
         \end{cases}
\end{equation}
где $f$ и $g$~--- произвольные неотрицательные неубывающие функции определённые
на промежутке $\left[1, +\infty \right]$. В
таблице~\ref{tab:f_g_examples} приведены примеры функций $f$ и $g$,
которые могут быть использованы.

\begin{table}[!h]
    \caption{Примеры функций $f$ и $g$ в обобщённой формуле TF-IDF}\label{tab:f_g_examples}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}\hline
    \boldmath$f(x)$ & $1$ & $\log{x}$ & $\log{x}$ & $\log{x}$ & $\sqrt{x}$ & $\sqrt{x}$ & $\sqrt{x}$ & $x$ & $x$ & $x$ \\\hline
    \boldmath$g(x)$ & $1$ & $1$ & $\log{x}$ & $\sqrt{x}$ & $1$ & $\log{x}$ & $\sqrt{x}$ & $1$ & $\log{x}$ & $\sqrt{x}$ \\\hline
\end{tabular}
\end{table}

Ещё одним распространённым способом вычисления коэффициентов
матрицы <<термин-документ>> является формула \textit{log-entropy}.
Она имеет следующий вид:
\begin{equation}\label{eq:log_entropy}
    d_{ij} = \log{(\mathrm{tf}_{ij} + 1)} \cdot
    (1 + \sum_{j=1}^{n} \frac{p_{ij} \cdot \log{p_{ij}}}{\log{n}}),\quad
    p_{ij} = \frac{\mathrm{tf}_{ij}}{\mathrm{gf}_i}.
\end{equation}

Существуют также и другие известные способы вычисления элементов
матрицы <<термин-документ>>, но так как в рамках настоящего исследования
они не использовались, их рассмотрение будет опущено.

\section{Латентный семантический анализ}
\label{sec:latent_semantic_analysis}
\textit{Латентный семантический анализ}~--- это некоторый подход,
часто применяющийся в задачах анализа текста. Он основан на
предположении о том, что термины и документы могут быть отнесены
к некоторым тематикам, поэтому данный подход также называют
\textit{тематическим моделированием}.

Опишем задачу, которую решает латентный семантический анализ,
более формально. Пусть $D \in \mathbb{R}^{m \times n}$~--- матрица 
<<термин-документ>>, вычисленная каким-либо образом. Требуется
выполнить следующее разложение данной матрицы:
\[
    D = U \cdot V^T,\quad U \in \mathbb{R}^{m \times k},\quad V \in \mathbb{R}^{n \times k},
\]
где $U$~--- матрица <<термин-тема>>, $V$~--- матрица <<документ-тема>>,
а $k$~--- число тем. Строка матрицы $U$ под номером $i$ 
характеризует <<степень принадлежности>> термина $i$ 
каждой из тем. Строка матрицы $V$ под номером $j$ обозначает
<<степень принадлежности>> документа $j$ каждой из тем.
Можно сказать, что латентный семантический анализ решает
две задачи нечёткой кластеризации (fuzzy clustering)~--- для
терминов и для документов.

В виду того, что число терминов, как правило,
оказывается очень большим, латентный семантический анализ
применяется для более компактного представления 
документов в векторном пространстве. Кроме того, данный подход
применим для категоризации документов.

Алгоритмы, решающие задачу латентного семантического анализа,
можно разделить на две группы: вероятностные и остальные.

Алгоритмы, относящиеся к первой группе, реализуются на
основе вероятностной модели порождения данных, которая
формально записывается следующим образом:
\[
    p(d, w) = \sum_{t \in T} p(t)p(w|t)p(d|t),
\]
где $T$~--- множество тем, $p(d, w)$~--- вероятность возникновения
термина $w$ в документе $d$, $p(t)$~--- вероятность выбрать тему $t$,
$p(w|t)$~--- вероятность выбрать термин $w$ из темы $t$, а
$p(d|t)$~--- вероятность выбрать документ $d$, при условии, что
выбрана тема $t$.

В алгоритмах, основанных на вероятностной модели порождения
данных, элементами матрицы являются вероятности.

Одним из вероятностных алгоритмов латентного семантического анализа
является \textit{probabalistic latent semantic analysis}
(PLSA)~\cite{chemudugunta2007modeling}. Другим, наиболее
популярным вероятностным алгоритмом является \textit{latent
Dirichlet allocation} (LDA)~\cite{blei2003latent}.

Из алгоритмов латентного семантического анализа, не относящихся
к вероятностным, следует упомянуть 
\textit{latent semantic indexing}~\cite{deerwester1990indexing}.
Данный алгоритм реализован на основе сингулярного разложения,
поэтому он позволяет снижать размерность сильно разреженных
матриц (какими часто являются матрицы <<термин-документ>>) с
минимальным потерями информации. Достоинством данного алгоритма
является тот факт, что он не завязан на предположении о
существовании латентных тем. Главный же его недостаток~--- 
полученные темы являются неинтерпритируемыми для человека.
Вероятностные модели лишены этого недостатка.

\section{Векторное представление слов}
\label{sec:word_embedding}
\textit{Векторное представление слов} (word embedding)~---
техника, позволяющая преобразовать множество терминов в
численные векторы.

\chapterconclusion

Выводы выводы
